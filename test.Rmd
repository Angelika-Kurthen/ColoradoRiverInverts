---
title: "BaysianZINB_NZMS.R"
output: pdf_document
date: "2024-07-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries}
# load libraries needed
library(dataRetrieval)
library(lubridate)
require(rstan)
library(rv)


```


```{r load functions}
# function to index and summarize temperature data over timesteps length
TimestepTemperature <- function(temp){
  # Make an index to be used for aggregating
  ID <- as.numeric(as.factor(temp$Date))-1
  # want it to be every 14 days, hence the 14
  ID <- ID %/% 14
  # aggregate over ID and TYPE for all numeric data.
  outs <- aggregate(temp[sapply(temp,is.numeric)],
                    by=list(ID),
                    FUN=mean)
  # format output
  names(outs)[1:2] <-c("dts","Temperature")
  # add the correct dates as the beginning of every period
  outs$dts <- as.POSIXct(temp$Date[((outs$dts*14)+1)], origin = "1970-01-01")
  # order by date in chronological order
  temps <- outs[order(outs$dts),]
  
  return(temps)
}

```

```{r rstan specifications}
# make sure rstan can run in parallel
rstan_options(auto_write = TRUE)
options(mc.cores = min(c(parallel::detectCores(), 8)))

# specify run/iterations
nchains <-  min(c(parallel::detectCores(), 8))
niters <- 5000
nkeep <- 2500
nthin <- ceiling((niters/2)*nchains/nkeep)

```
## Project Goals

The overarching project that this is related to is a time-varying, stage structured population matrix model for New Zealand Mudsnails (and 2 or 3 other invertebrates in the same function feeding group) in the Colorado River below Glen Canyon Dam. We want to use these models to run some climate change and dam management scenarios to show how changes in flow/temp impact the invertebrates. Then we want to couple those models together to create a "community" model and see how changes in management can affect the community (or better specified, assemblage of invertebrates). 

Below is an image that shows how the Lefkovitch matrix for the time-varying stage structured population model (the mathematical model) is modified. The values and the equations that modify them are pulled from the literature, choosing literature that is outside of the emprical data we will use to validate (so basically all literature about the organism except for literature about NZ Mudsnails in the Colorado River below Glen Canyon Dam).
!["Time-Varying Matrix Population Model"](TimeVaryingMatrix.png)
Given a matching time series of river flows, temperatures, and if relevant, a time series of hydropeaking intensities, we can model the abundance of organisms for each 2 week time step in that time series. 

The goal is to force either real or simulated flow, temperature, and hydropeaking time series through the model to show how invertebrate abundances may change under those different environmental scenarios. We are aiming for a more qualitative understanding (as in, do the organisms increase, decrease, or stay the same under different scenarios), as there isn't a need to precise numbers. The goal is not to tell the audience how many snails or invertebrates are in the system, just if they will increase or decrease under different flow, temperature, and hydropeaking regimes. 

Since we have the model framework and the goal for the model, we then need to validate the model to demonstrate that it is acceptable to use. 

We can do this by forcing real time series of river flow, temperature, and hydropeaking regime through the model to get estimates of abundances. Then we can compare the model estimates to real empirical data gathered during that time. 

Since we are uninterested in numbers, just qualitative assessment, we can use a Spearman correlation, which is a ranked correlation to compare model estimates and empirical data for the time series of environmental data. When we do that, we get a correlation of around 0.6. This is on par with other mechanistic models looking at population dynamics (REFS HERE). So we can justify the use of the model, even just comparing the model estimates to the raw empirical data. 

## Invasive New Zealand Mudsnail Empirical Data

We have some empirical NZ Mudsnail Data from the Colorado River below Glen Canyon Dam, taken from many different days between 2008 and 2020. Data was collected via a drift sampling method, where a net is thrown over the boat. We have data on the date samples were taken, the count, some info on the size of the snails, and the volume of water that was sampled. 

```{r NZMS samp}
NZMSsamp <- read.csv("~/ColoradoRiverInverts/NZMSsamp.csv")
View(NZMSsamp)
```


```{r range, echo = F}
# makes sure dates are in date form
NZMSsamp$Date <- as.Date(NZMSsamp$Date)

# remove any samples that are flagged as "strange"
NZMSsamp <- NZMSsamp[which(NZMSsamp$FlagStrange == F),]


# we have a pretty large range of counts
range(NZMSsamp$CountTotal)

# number of zeros in the data
length(NZMSsamp$SampleNumber[which(NZMSsamp$CountTotal == 0)])
```
There are 189 zero counts in the data set, which is a good amount. We can also see that in a histogram of the count data, it is inflated by low numbers, with a few large stragglers.
```{r hist}
# plot frequency of counts 
hist(NZMSsamp$CountTotal, breaks = 200, col = "red", border = "darkred")
```

There is also quite a bit of variation over time, but clearly some phenology with yearly pulses. This means that we might want to use sample month as a covariate.  
```{r}
# plot count over time
plot(NZMSsamp$Date, NZMSsamp$CountTotal)
```

We have a pretty uneven sampling distribution (you can see by the spacing of points above). Some days only have 1 data point, other have 10. Some months are more heavily sampled than others. Because of this, we might want to bin out dates, using 2 weeks (which happens to be the length of our mathematical model timestep). We can do this by indexing our count dates. We also will make a column for sample month, so we can use that as a predictor in our analysis. We can then continue to explore sample frequency. 
```{r}
library()
# read in temperature data - optional
temp <- readNWISdv("09380000", "00010", "2007-10-01","2023-05-01")

# put temp data into 2 week timestep bins
temps <- TimestepTemperature(temp)

# we have uneven sample distribution (some days, 11 samples taken, others only 1)
# therefore, we use 2 week timesteps

# we now will index each invert sample based on which timestep it falls into
vals <- vector()
for (i in 1:length(temps$dts)){
  d <- NZMSsamp[which(NZMSsamp$Date %within% interval(temps$dts[i], temps$dts[i+1]-1) == T),]
  if (length(d$CountTotal) > 0) {
    s<- rep(i, times = length(d$CountTotal))
    vals <- append(vals, s)}
}

# phenology may also play into dynamics, so include month column as well
month <- month(NZMSsamp$Date)

# add to data frame
NZMSsamp <- cbind(NZMSsamp, vals, month)

# now we need into include mean water temperature for each 
NZMSsamp <- cbind(NZMSsamp, temps$Temperature[NZMSsamp$vals])
```

Sample freqency is fairly similar, other than December (probably because people aren't sampling during the holidays at the end of the month)
```{r}
# sample frequency by month
hist(NZMSsamp$month)
```

There are definitely some years that are more heavily sampled than others. In fact, it seems like sampleing has dwindled over the years. 
```{r}
# sample frequency by year 
hist(year(NZMSsamp$Date))
```
We can also divide our data up into 2 week time steps (12 years * 26 fortnights per year) and look at the frequency.

```{r}
# sample density by two week timestep
hist(NZMSsamp$Date, breaks = 312)

```
Unfortunately this records the y axis as density, not sample frequency, which is what happens when the data are in date format for some reason. We can get the actual sample frequencies by converting the Date to a numeric value. 
```{r}
hist(as.numeric(NZMSsamp$Date), breaks = 312)
```
As you can see, some timesteps have over 40 samples taken, some only 1, some 0. It is very uneven, overall. 

## Fitting a distribution to account for imperfect sampleing 

While we have a "good enough" fit for our model output (about 0.6 which is ok for math models, at least in the literature) and our raw empirical data. It's been suggested that because we have imperfect detection, the empirical count data might have lower numbers than the actual abundances of NZ mudsnails and potentially more 0s than there really are in the system (knowing that at any given point there is probably a snail since they are highly invasive and also highly abundant).

However, detection probability is unknown and probably changes from sample event to sample event. Because both detection probability AND the actual abundance are both unknowns. 


The one way to solve this is to use an N-mixture model. The traditional n-mix model for count data with repeated measures (we have count data with repeated measures within each 2 week timestep we want to calculate the actual mean abundance for). 

Our count data is drawn from a binomial distribution based on the probability of detecting mudsnails and the actual abundance of mudsnails at timestep t. 

**OurCountData(t) ~ Binom(DetectionProbability(t), ActualAbundance(t))**

We draw our detection probabilities from a beta distribution of 1,1 (basically a uniform distribution)

**DetectionProbability(t) ~ beta(1,1)**

We assume that the ActualAbundance at timestep t is pulled from a Poisson distribution with $\lambda$(t) equal to the mean abundance at timestep t. 

**ActualAbundance ~ Pois(MeanAbundance(t))**

That MeanAbundance(t) is pulled from a uniform distribution between 0 and the maximum Count possible from the data + a little more. 

**MeanAbundance(t) ~ Unif(0, max(Count*1.25))**


However, because Count ~ DetectionProbability* ActualAbundance, it seems like pulling the two values apart might not be possible - that is, how can we accurately estimate ActualAbundance without knowing DetectionProbability without landing on multiple solutions that fit the data. 

One way to solve this is to assume a global DetectionProbability, BUT we know this isn't the case. Certain environmental conditions (increased flow) can cause more mudsnails to enter the drift, this increasing the likelihood of actually catching one in the net and detecting it. This assumption does not seem very reasonable. This leads us back to our Count = DetectionProbability*ActualAbundance problem and leads me to wonder if we really can disentangle the two.

So we have to move on to fitting the data to a probability distribution. From our data, we know that different volumes of water were sampled though the nets, so we need to offset our count data by net volume. We also have some data (like flow), which impacts how many NZ mudsnails are in the water column (see Kennedy et al. 2016). Additionally, if there is a phenological aspect, we want to also account for month as a predictor. We start with a negative binomial distribution with predictors.  

Equations Neg Binom Regression

Count ~ NegBinom(p, r)
where p = r/(r + mu[i])
log(mu[i]) = b0[ts[i]] + X[i]*beta + offset[i]
OR #random intercept
log(mu[i]) = b0[timestep[i]]+ b1* TimeElapsed + b2* Flow + b3 * Month + offsetVolume[i]

Perhaps stick to raw temps instead of Month
Because I have nested data - multiple data points per timestep - we might also need to include a linear predictor for timestep to account for the impact of it. This was suggested by a USGS researcher working with yearly timeseries data for fish, because in each year multiple fish are caught at different times within each year and are binned to the year.  

#month is circular -> pkg circular Julian Date/(2pi/365) and or Categorical 

Correlated predictors? Because we are predicting we only want to get a good estimate of mu, don't need to worry about the coefficients 
Closure - 

Stan
```{r}
##############################################
nb_sturg2 <-"
data {
  int<lower=1> N;      // number of observations
  int<lower=1> Ka;       // number of model coefficients
  int<lower=1> Nts;     // number of timesteps
  int<lower=1,upper=Nts> ts[N];   // timestep hierarchy
  int<lower=0> Y[N];  // response variable 

  matrix[N, Ka] X;    // count model design matrix

  vector[N] offset;   // exposure variable (Volume)
  
  }

parameters {
    vector[Ka] beta;      // model coefficients
    real<lower=0> r;      // inverse disperson parameter
    vector[Nts] b0;       // count model timestep specific intercept
    real mu_b;            // hyper mean for b0
    real<lower=0> tau_b;  // hyper sd for b0
  }
  
transformed parameters{
  vector[N] eta;         // initialize positive count (stand                          in for log mu) predictor term for eta

  for (i in 1:N){          // linear predictor for  counts
    eta[i] = b0[ts[i]]+ X[i] * beta + offset[i];
  }
  }
  
model {
  // priors
  beta ~ normal(0,5);           // count model coefficients
  r ~ gamma(0.001, 0.001);        // inverse dispersion parameter
  
  b0 ~ normal(mu_b, tau_b);     // timestep specific count model intercepts
  mu_b ~ normal(0,5);           // hyper mean for b0
  tau_b ~ normal(0,5);          // hyper sd for b0

  // likelihood
    
for(i in 1:N){ 
    target += neg_binomial_2_log_lpmf(Y[i] | eta[i], r); 
    } 
}
"
 
# put the model into stan
fit_nb_sturg2 <- stan_model(model_code = nb_sturg2)

# bundle data, initial values, n.chains, parameters
NB_in <- function(data = NZMSsamp, n.chains = nchains){
  y <- data$CountTotal # count data
  tmp <- !is.na(y) #no NAs
  data <- data[tmp,] #remove NAs
  N <- dim(data)[1] # get length of count data 
  Y <- y[tmp] # count data without NAs
  X <- cbind(data$TimeElapsed - mean(data$TimeElapsed), #timeelapsed (higher chance of more counts the longer in water)
             data$X_00060_00003 - mean(data$X_00060_00003), #flow in cfs
             data$month - mean(data$month)) # month
  Kbeta <- dim(X)[2] # number of predictor variables
  offsets <- log(data$Volume) # want to offset by log Volume filtered
  timestp <- as.numeric(ordered(data$vals)) #ordered list of timesteps
  nts <- length(unique(vals)) # number of timesteps
  data <- list(N = N, Nts = nts, Ka = Kbeta, X = X, Y = Y, offset = offsets, ts = timestp)
  inits <- list()
  for(i in 1:n.chains)
    inits[[i]] <- list(b0 = rnorm(nts), beta = rnorm(Kbeta), r = runif(1), tau_b = runif(1))
  paras <- c("b0", "beta", "r", "tau_b", "mu_b")
  return(list(data = data, init = inits, nchains = n.chains, para = paras))
}

# put into stan
input.to.stan <- NB_in()

## fun model --> run 
keep_nb_sturg2 <- sampling(fit_nb_sturg2, data=input.to.stan$data,
                             init=input.to.stan$init,
                             pars=input.to.stan$para,
                             iter=niters,thin=nthin,
                             chains=input.to.stan$nchains)
```

```{r}
sink("N-mixturePoisNZMS.jags")
cat("
model{
    # Priors
    for(i in 1:nAlpha){ # nAlpha is the number of site predictor variables
      alpha[i] ~ dnorm(0, 0.1) # alphas are the site covariates
    }

    for(i in 1:nBeta){ # nBeta is number of site x observation predictors
      beta[i] ~ dnorm(0, 0.1) # betas are the observation covariates
    }

    # Likelihood
    for(r in 1:R){
     N[r] ~ dpois(lambda[r]) #start with pulling from Poisson
     log(lambda[r]) <- sum(alpha * XN[r, ]) #XN is matrix of site covariates

     for(j in 1:J){
      y[r, j] ~ dbinom(p[r, j], N[r]) #binomial for observed counts
      logit(p[r, j]) <- sum(off[r, j] + beta * Xp[r,j,]) #XP(obs covariates)
     }
    }
    
    } # End model
", fill = TRUE)
sink()
```

```{r}
# observations need to be in RxJ matrix, where R is # of sites and J is max number of obs per site
# we measured over temps$dts (those are our timesteps)
                   
max_visits <- vector() # vector to put max obs per site
means <- vector()
for (i in 1:length(temps$dts)){  # cycle through all the 14 day timesteps that we have model output for
  # pull abundances between each each timestep
  d <- NZMSsamp[which(NZMSsamp$Date >= temps$dts[i] & NZMSsamp$Date < temps$dts[i+1]), ]
  max_visits[i] <- length(d$Date) # number of observations
  means[i] <- mean(d$CountTotal) # means - this is just for checking NAs
}

# make data frame with the timestep, the mean
df <- cbind.data.frame(temps$dts, means)
# remove anythig where there are NA values of Density (means that volume or count is missing)
df <- df[!is.na(df$means), ]

# define our RxJ matrix
R <- length(temps$dts)
J <- max(max_visits)
site_mat <- matrix(data = NA, nrow = R, ncol = J)
dens_mat <- matrix(data = NA, nrow = R, ncol = J)
obs_intercept <- matrix(data = 1, nrow = R, ncol = J)
# make vector for flows at each timestep
# make RxJ matrix full of densities
# make RxJ matrix full of raw counts
# make RxJ matrix full of volumes sampled for each abundance
flows <- vector()
temperature <- vector()
volumes <- matrix(data = NA, nrow = R, ncol = J)
time <- matrix(data = NA, nrow = R, ncol = J)
for (i in 1:length(temps$dts)){
  d <- NZMSsamp[which(NZMSsamp$Date >= temps$dts[i] & NZMSsamp$Date < temps$dts[i+1]), ]
  flows[i] <- mean(d$X_00060_00003)
  temperature[i] <- mean(d$`temps$Temperature[NZMSsamp$vals]`)
  time[i,] <- c(d$TimeElapsed, rep(NA, times = (J- length(d$CountTotal))))
  site_mat[i, ] <- c(d$CountTotal, rep(NA, times = (J- length(d$CountTotal))))
  dens_mat[i, ] <- c(as.integer(d$Density), rep (NA, times = (J - length(d$Density))))
  volumes[i, ] <- c((d$Volume),rep(NA, times = (J- length(d$CountTotal))))
}

# we need to remove all timesteps that are just NAs
nodata <- which(is.na(site_mat[,1]))
# first identify all the timsteps that don't have data (so we can match them up later)
site_mat <- as.matrix(site_mat[-nodata,]) # count data
dens_mat <- as.matrix(dens_mat[-nodata, ]) # density data
obs_intercept <- as.matrix(obs_intercept[-nodata,]) # intercept for obs cov
time <- as.matrix(scale(time[-nodata,])) # duration in H20 obs cov
time[is.na(time)] <- mean(time, na.rm = TRUE) # replace NAs with mean duration time since NAs not allowed in predictors
flows <- as.data.frame(scale(flows[-nodata])) # site cov flow 
temperature <- as.data.frame(scale(temperature[-nodata])) # site cov temp
# dimnames(time) <- list(temps$dts[-nodata], seq(1:48))
# time <- list(time)
# names(time) <- c("time")
site_intercept <- rep(1, times = length(flows$V1))
site_covs<- as.matrix(cbind(site_intercept, flows, temperature))
obs_covs <- array(data= NA, dim = c(length(flows$V1),J,2))
obs_covs[,,1] <- obs_intercept
obs_covs[,,2] <- time
#offset
offset <- as.matrix(log(volumes[-nodata, ]))
offset[is.na(offset)] <- mean(offset, na.rm = TRUE) # replace NAs with mean duration time since NAs not allowed in predictors or offsets
# dimnames(volumes) <- list(temps$dts[-nodata], seq(1:48))
# volumes <- list(volumes)
# names(volumes) <- c("vol")


jags_data <- list(y = (site_mat),
                  XN = site_covs,
                  Xp = (obs_covs),
                  J = dim(site_mat)[2], #visits
                  R = dim(site_mat)[1], #sites
                  off = (offset), #obs offset
                  nAlpha = dim(site_covs)[2],
                  nBeta = dim(obs_covs)[3])

jags_inits <- function(){
  list(
  N = apply(jags_data$y, 1, max, na.rm=TRUE),
  alpha=runif(nAlpha,-1,1),
	beta=runif(nBeta,-1,1))}

parameters <- c("alpha", "beta", "lambda", "p", "N")

nc <- 3
ni <- 10000
nb <- 2500
nt <- 1

Nmix_fit <- jagsUI::jags(data = jags_data, inits = jags_inits,
                    parameters.to.save = parameters, model.file = "N-mixturePoisNZMS.jags",
                    n.chains = nc, n.iter = ni, n.burnin = nb, n.thin = nt, parallel = T)

print(Nmix_fit)
```



```{r}
library(rstanarm)
nb_mod <- stan_glmer(CountTotal ~ month + TimeElapsed + X_00060_00003 + (1|vals),
             data = NZMSsamp, family = neg_binomial_2,
          offset = Volume,
          prior_intercept = normal(0,5),
          prior = normal(0,1), 
          prior_aux = dgamma(0.01, 0.01), 
          chains = 8, iter = 5000*2, seed = 123
          )

tidy(nb_mod, effects = "fixed", conf.int = T, conf.level= 0.8)
prediction_summary(model = nbmod, data = NZMSsamp)
```